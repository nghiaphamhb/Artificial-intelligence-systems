# MNIST Neural Network From Scratch ğŸ§ âœï¸

This project implements a full neural-network training pipeline **without using PyTorch layers** such as `nn.Linear`, `nn.Conv2d`, `nn.MaxPool2d`, or `nn.Dropout`.
All components â€” Linear, ReLU, Dropout, Conv2d, MaxPool2D, Flatten, Softmax, and Adam â€” are **built manually** using only tensor operations + autograd.

---

## Features ğŸš€

### **âœ” Manual Deep Learning Layers**

* Custom `Linear`
* Custom `Conv2d` (loop-based)
* Custom `MaxPool2D`
* `Dropout`, `ReLU`, `Flatten`
* `SoftmaxCrossEntropy`
* Custom `AdamOptimizer`

No neural-network modules from `torch.nn` are used.

### **âœ” Manual MNIST Loader**

Reads raw `.idx` files using `struct`:

* 60,000 training images
* 10,000 test images
* 28Ã—28 grayscale

### **âœ” Four Models Implemented**

1. **MLP 1** â€” Basic Dense Network
2. **MLP 2** â€” Larger + Dropout
3. **CNN 1** â€” Custom Conv + Pool
4. **CNN 2** â€” Larger CNN

Each model trains separately and logs performance.

### **âœ” Training Visualizations**

Plots for all models:

* Training Loss
* Validation Loss
* Training Accuracy
* Validation Accuracy

This allows easy model comparison.

### **âœ” Handwritten Digit Prediction âœï¸â¡ï¸ğŸ”¢**

A final model can:

* Accept an image drawn in Paint/GIMP
* Resize â†’ grayscale â†’ invert â†’ normalize
* Predict the digit

### **Dataset**

Link to dataset: https://www.kaggle.com/datasets/hojjatk/mnist-dataset/code/data

---

## Why This Project? ğŸ¯

To explore **how neural networks work internally**, without relying on high-level PyTorch layers.
It is a learning-focused implementation meant to teach:

* Tensor transformations
* Convolution mechanics
* Optimization
* Model comparison
* Practical evaluation

---

## Requirements ğŸ“¦

```
Python 3.8+
PyTorch (tensor + autograd only)
NumPy
Matplotlib
```

---

## Structure ğŸ“

```
/data           # MNIST idx files
notebooks/      # Training pipeline
README.md
main.ipynb
```
