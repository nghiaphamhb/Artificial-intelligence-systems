{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lliw2zfTjcjh"
      ],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import lib"
      ],
      "metadata": {
        "id": "qhevyErIiZi2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "scXE7i0cVTPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9c02e11-8ff8-4471-b521-d2eaa87b29de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c15b05c4f70>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "from typing import Optional, List, Tuple, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    r2_score,\n",
        ")\n",
        "\n",
        "# Choose device for tensors\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "# For reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom impurity functions + regression metrics"
      ],
      "metadata": {
        "id": "OHoMzscvigMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Gini impurity for classification target y.\n",
        "def gini_impurity(y: torch.Tensor) -> float:\n",
        "    # If node is empty, impurity is zero by definition\n",
        "    if y.numel() == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Get unique classes and their counts\n",
        "    classes, counts = torch.unique(y, return_counts=True)\n",
        "    probs = counts.float() / y.numel()\n",
        "\n",
        "    gini = 1.0 - torch.sum(probs ** 2)\n",
        "    return float(gini.item())\n",
        "\n",
        "# Compute entropy impurity for classification target y.\n",
        "def entropy_impurity(y: torch.Tensor, eps: float = 1e-12) -> float:\n",
        "    if y.numel() == 0:\n",
        "        return 0.0\n",
        "\n",
        "    classes, counts = torch.unique(y, return_counts=True)\n",
        "    probs = counts.float() / y.numel()\n",
        "    probs = torch.clamp(probs, min=eps)\n",
        "\n",
        "    ent = -torch.sum(probs * torch.log2(probs))\n",
        "    return float(ent.item())\n",
        "\n",
        "# Compute MSE-based impurity for regression node.\n",
        "def mse_impurity(y: torch.Tensor) -> float:\n",
        "    if y.numel() == 0:\n",
        "        return 0.0\n",
        "\n",
        "    y = y.float()\n",
        "    mean_y = torch.mean(y)\n",
        "    mse = torch.mean((y - mean_y) ** 2)\n",
        "    return float(mse.item())\n"
      ],
      "metadata": {
        "id": "utOAFVR6g2--"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regression_metrics(y_true, y_pred):\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    mse = np.mean((y_true - y_pred) ** 2)\n",
        "    mae = np.mean(np.abs(y_true - y_pred))\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "    r2 = 1.0 - ss_res / ss_tot if ss_tot > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"MAE\": float(mae),\n",
        "        \"MSE\": float(mse),\n",
        "        \"RMSE\": float(rmse),\n",
        "        \"R2\": float(r2),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "zZwrjOmSKBM6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test functions"
      ],
      "metadata": {
        "id": "lliw2zfTjcjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick test for impurity functions\n",
        "y_cls = torch.tensor([0, 0, 1, 1, 1])\n",
        "y_reg = torch.tensor([1.0, 2.0, 3.0])\n",
        "\n",
        "print(\"Gini:\", gini_impurity(y_cls))\n",
        "print(\"Entropy:\", entropy_impurity(y_cls))\n",
        "print(\"MSE:\", mse_impurity(y_reg))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-9gLuAzjayA",
        "outputId": "468ea379-8cd5-4c7f-c358-8f615667eea5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gini: 0.47999995946884155\n",
            "Entropy: 0.9709506034851074\n",
            "MSE: 0.6666666865348816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision tree:"
      ],
      "metadata": {
        "id": "65imDUG-jV3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "    def __init__(\n",
        "        self,\n",
        "        classification: bool = True,\n",
        "        max_depth: int = 10,\n",
        "        min_samples_split: int = 2,\n",
        "        min_samples_leaf: int = 1,\n",
        "        criterion: str = \"gini\",   # or \"entropy\" or \"mse\"\n",
        "        verbose: bool = False,     # debug flag\n",
        "        max_features_split: int | None = None,   # limit features per node\n",
        "      max_thresholds: int = 32,                # limit thresholds per feature\n",
        "    ):\n",
        "        self.classification = classification\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.criterion = criterion\n",
        "        self.tree_ = None\n",
        "        self.device = DEVICE\n",
        "        self.n_features_ = None\n",
        "\n",
        "        # Debug-related fields\n",
        "        self.verbose = verbose       # if True, print debug info\n",
        "        self._node_count = 0         # count number of nodes in tree\n",
        "\n",
        "        self.max_features_split = max_features_split\n",
        "        self.max_thresholds = max_thresholds\n",
        "\n",
        "    # Impurity dispatch\n",
        "    def _impurity(self, y: torch.Tensor) -> float:\n",
        "        if self.classification:\n",
        "            if self.criterion == \"gini\":\n",
        "                return gini_impurity(y)\n",
        "            elif self.criterion == \"entropy\":\n",
        "                return entropy_impurity(y)\n",
        "            else:\n",
        "                raise ValueError(\"Unknown criterion for classification.\")\n",
        "        else:\n",
        "            # regression\n",
        "            return mse_impurity(y)\n",
        "\n",
        "    # Stopping condition: check if node is pure\n",
        "    def _is_pure(self, y: torch.Tensor) -> bool:\n",
        "        if self.classification:\n",
        "            return torch.unique(y).numel() == 1\n",
        "        else:\n",
        "            return torch.var(y.float()) < 1e-8\n",
        "\n",
        "    # Leaf value (prediction stored in leaf)\n",
        "    def _leaf_value(self, y: torch.Tensor):\n",
        "        if self.classification:\n",
        "            classes, counts = torch.unique(y, return_counts=True)\n",
        "            idx = torch.argmax(counts)\n",
        "            return int(classes[idx].item())\n",
        "        else:\n",
        "            return float(torch.mean(y).item())\n",
        "\n",
        "    # Find best split across all features & thresholds\n",
        "    def _best_split(self, X: torch.Tensor, y: torch.Tensor):\n",
        "        num_samples, num_features = X.shape\n",
        "        if num_samples < self.min_samples_split:\n",
        "          return None, None, None, None\n",
        "\n",
        "        parent_impurity = self._impurity(y)\n",
        "\n",
        "        best_gain = 0.0\n",
        "        best_feat = None\n",
        "        best_thresh = None\n",
        "        best_left_mask = None\n",
        "        best_right_mask = None\n",
        "\n",
        "        # Decide which features to try at this node\n",
        "        if self.max_features_split is not None and self.max_features_split < num_features:\n",
        "            # random subset of features\n",
        "            feat_indices = torch.randperm(num_features, device=X.device)[: self.max_features_split]\n",
        "        else:\n",
        "            feat_indices = torch.arange(num_features, device=X.device)\n",
        "\n",
        "        for feat_idx in feat_indices:\n",
        "            feature_values = X[:, feat_idx]\n",
        "\n",
        "            thresholds = torch.unique(feature_values)\n",
        "\n",
        "            # Limit number of thresholds to test (for speed)\n",
        "            if thresholds.numel() > self.max_thresholds:\n",
        "                # random sample of thresholds\n",
        "                perm = torch.randperm(thresholds.numel(), device=X.device)[: self.max_thresholds]\n",
        "                thresholds = thresholds[perm]\n",
        "\n",
        "            for t in thresholds:\n",
        "                left_mask = feature_values <= t\n",
        "                right_mask = ~left_mask\n",
        "\n",
        "                if left_mask.sum().item() < self.min_samples_leaf:\n",
        "                    continue\n",
        "                if right_mask.sum().item() < self.min_samples_leaf:\n",
        "                    continue\n",
        "\n",
        "                y_left = y[left_mask]\n",
        "                y_right = y[right_mask]\n",
        "\n",
        "                n_left = y_left.numel()\n",
        "                n_right = y_right.numel()\n",
        "\n",
        "                impur_left = self._impurity(y_left)\n",
        "                impur_right = self._impurity(y_right)\n",
        "\n",
        "                weighted_imp = (\n",
        "                    n_left / num_samples * impur_left\n",
        "                    + n_right / num_samples * impur_right\n",
        "                )\n",
        "\n",
        "                gain = parent_impurity - weighted_imp\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feat = int(feat_idx.item())  # feat_idx is tensor\n",
        "                    best_thresh = float(t.item())\n",
        "                    best_left_mask = left_mask\n",
        "                    best_right_mask = right_mask\n",
        "\n",
        "            return best_feat, best_thresh, best_left_mask, best_right_mask\n",
        "\n",
        "\n",
        "    # Recursively build the tree\n",
        "    def _build_tree(self, X: torch.Tensor, y: torch.Tensor, depth: int):\n",
        "        num_samples = X.shape[0]\n",
        "\n",
        "        # Count node creation for debug\n",
        "        self._node_count += 1\n",
        "\n",
        "        # stopping conditions\n",
        "        if (\n",
        "            depth >= self.max_depth\n",
        "            or num_samples < self.min_samples_split\n",
        "            or self._is_pure(y)\n",
        "        ):\n",
        "            return {\n",
        "                \"is_leaf\": True,\n",
        "                \"value\": self._leaf_value(y),\n",
        "            }\n",
        "\n",
        "        feat_idx, thresh, left_mask, right_mask = self._best_split(X, y)\n",
        "\n",
        "        # if no valid split\n",
        "        if feat_idx is None:\n",
        "            return {\n",
        "                \"is_leaf\": True,\n",
        "                \"value\": self._leaf_value(y),\n",
        "            }\n",
        "\n",
        "        X_left, y_left = X[left_mask], y[left_mask]\n",
        "        X_right, y_right = X[right_mask], y[right_mask]\n",
        "\n",
        "        return {\n",
        "            \"is_leaf\": False,\n",
        "            \"feature\": feat_idx,\n",
        "            \"threshold\": thresh,\n",
        "            \"left\": self._build_tree(X_left, y_left, depth + 1),\n",
        "            \"right\": self._build_tree(X_right, y_right, depth + 1),\n",
        "        }\n",
        "\n",
        "    # Public API: fit model\n",
        "    def fit(self, X: torch.Tensor, y: torch.Tensor):\n",
        "        X = X.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        self.n_features_ = X.shape[1]\n",
        "        self._node_count = 0  # reset counter\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"[DT] Start building tree: n_samples={X.shape[0]}, \"\n",
        "                  f\"n_features={self.n_features_}\")\n",
        "\n",
        "        self.tree_ = self._build_tree(X, y, depth=0)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"[DT] Finished building tree, total nodes: {self._node_count}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    # Predict one sample\n",
        "    def _predict_one(self, x: torch.Tensor):\n",
        "        \"\"\"Traverse the tree for a single sample.\"\"\"\n",
        "        node = self.tree_\n",
        "        while not node[\"is_leaf\"]:\n",
        "            feat = node[\"feature\"]\n",
        "            thresh = node[\"threshold\"]\n",
        "            if x[feat] <= thresh:\n",
        "                node = node[\"left\"]\n",
        "            else:\n",
        "                node = node[\"right\"]\n",
        "        return node[\"value\"]\n",
        "\n",
        "    # Predict batch\n",
        "    def predict(self, X: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Predict for a batch X: [n_samples, n_features].\n",
        "        Returns numpy array of predictions.\n",
        "        \"\"\"\n",
        "        X = X.to(self.device)\n",
        "        preds = []\n",
        "        for i in range(X.shape[0]):\n",
        "            preds.append(self._predict_one(X[i]))\n",
        "        return np.array(preds)\n"
      ],
      "metadata": {
        "id": "z3WiHBqY9ktp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "6i4k3ZvzzXEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForest:\n",
        "    def __init__(\n",
        "        self,\n",
        "        classification: bool = True,\n",
        "        n_trees: int = 10,\n",
        "        max_depth: int = 10,\n",
        "        min_samples_split: int = 2,\n",
        "        min_samples_leaf: int = 1,\n",
        "        max_features: Optional[int] = None,  # number of features per tree\n",
        "        bootstrap: bool = True,\n",
        "        random_state: int = 42,\n",
        "        verbose: bool = False,              # debug flag\n",
        "    ):\n",
        "        self.classification = classification\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features = max_features\n",
        "        self.bootstrap = bootstrap\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.trees: List[DecisionTree] = []\n",
        "        self.features_per_tree: List[torch.Tensor] = []  # store feature indices\n",
        "        self.device = DEVICE\n",
        "\n",
        "        # Set seeds for reproducibility\n",
        "        np.random.seed(self.random_state)\n",
        "        torch.manual_seed(self.random_state)\n",
        "\n",
        "    # Internal: create bootstrap sample\n",
        "    def _bootstrap_sample(\n",
        "        self, X: torch.Tensor, y: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        if self.bootstrap:\n",
        "            # Sample with replacement\n",
        "            indices = torch.randint(\n",
        "                low=0,\n",
        "                high=n_samples,\n",
        "                size=(n_samples,),\n",
        "                device=X.device,\n",
        "            )\n",
        "        else:\n",
        "            # Use all samples without resampling\n",
        "            indices = torch.arange(n_samples, device=X.device)\n",
        "\n",
        "        return X[indices], y[indices]\n",
        "\n",
        "    # Fit the forest\n",
        "    def fit(self, X: torch.Tensor, y: torch.Tensor):\n",
        "        X = X.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Decide how many features each tree will see\n",
        "        if self.max_features is None:\n",
        "            max_feats = n_features\n",
        "        else:\n",
        "            max_feats = min(self.max_features, n_features)\n",
        "\n",
        "        self.trees = []\n",
        "        self.features_per_tree = []\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"[RF] Start training forest with {self.n_trees} trees, \"\n",
        "                  f\"{n_samples} samples, {n_features} features, \"\n",
        "                  f\"max_features per tree = {max_feats}\")\n",
        "\n",
        "        for i in range(self.n_trees):\n",
        "            if self.verbose:\n",
        "                print(f\"[RF] Training tree {i+1}/{self.n_trees} ...\")\n",
        "\n",
        "            # Sample feature indices for this tree\n",
        "            feat_indices = torch.randperm(n_features, device=X.device)[:max_feats]\n",
        "            self.features_per_tree.append(feat_indices)\n",
        "\n",
        "            # Bootstrap samples for this tree\n",
        "            X_boot, y_boot = self._bootstrap_sample(X[:, feat_indices], y)\n",
        "\n",
        "            # Create and train a DecisionTree\n",
        "            tree = DecisionTree(\n",
        "                classification=self.classification,\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                min_samples_leaf=self.min_samples_leaf,\n",
        "                criterion=\"entropy\" if self.classification else \"mse\",\n",
        "                verbose=False,\n",
        "                max_features_split=128,\n",
        "                max_thresholds=48,\n",
        "            )\n",
        "\n",
        "            tree.fit(X_boot, y_boot)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f\"[RF] Done tree {i+1}/{self.n_trees}\")\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"[RF] Finished training all trees.\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    # Predict for batch\n",
        "    def predict(self, X: torch.Tensor):\n",
        "        X = X.to(self.device)\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        if len(self.trees) == 0:\n",
        "            raise RuntimeError(\"RandomForest has not been fitted yet.\")\n",
        "\n",
        "        # Collect predictions of all trees\n",
        "        all_preds = []\n",
        "\n",
        "        for tree, feat_idx in zip(self.trees, self.features_per_tree):\n",
        "            X_sub = X[:, feat_idx]              # same feature subset as training\n",
        "            preds = tree.predict(X_sub)         # numpy array\n",
        "            all_preds.append(preds)\n",
        "\n",
        "        # Shape: [n_trees, n_samples]\n",
        "        all_preds = np.stack(all_preds, axis=0)\n",
        "\n",
        "        if self.classification:\n",
        "            # Majority vote along axis=0\n",
        "            final_preds = []\n",
        "            for i in range(n_samples):\n",
        "                values, counts = np.unique(all_preds[:, i], return_counts=True)\n",
        "                final_class = values[np.argmax(counts)]\n",
        "                final_preds.append(final_class)\n",
        "            return np.array(final_preds)\n",
        "        else:\n",
        "            # Regression: average predictions\n",
        "            return np.mean(all_preds, axis=0)"
      ],
      "metadata": {
        "id": "xJfNhAwJFe5b"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting"
      ],
      "metadata": {
        "id": "Sk8o3yJB7EUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientBoostingRegressor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_estimators: int = 50,\n",
        "        learning_rate: float = 0.1,\n",
        "        max_depth: int = 3,\n",
        "        min_samples_split: int = 2,\n",
        "        min_samples_leaf: int = 1,\n",
        "        max_features_split: Optional[int] = None,  # pass to inner DT\n",
        "        max_thresholds: int = 32,                 # pass to inner DT\n",
        "        random_state: int = 42,\n",
        "        verbose: bool = False,                    # debug flag\n",
        "    ):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features_split = max_features_split\n",
        "        self.max_thresholds = max_thresholds\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.device = DEVICE\n",
        "        self.trees: List[DecisionTree] = []\n",
        "        self.init_value_: float = 0.0  # F0(x) = mean(y)\n",
        "\n",
        "        np.random.seed(self.random_state)\n",
        "        torch.manual_seed(self.random_state)\n",
        "\n",
        "    # Fit the boosting model\n",
        "    def fit(self, X: torch.Tensor, y: torch.Tensor):\n",
        "        X = X.to(self.device)\n",
        "        y = y.to(self.device).float()\n",
        "\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Initial prediction: mean of y\n",
        "        self.init_value_ = float(torch.mean(y).item())\n",
        "\n",
        "        # Current prediction F(x) for all training samples\n",
        "        F_current = torch.full((n_samples,), self.init_value_, device=self.device)\n",
        "\n",
        "        self.trees = []\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"[GB] Start training: n_estimators={self.n_estimators}, \"\n",
        "                  f\"max_depth={self.max_depth}, lr={self.learning_rate}\")\n",
        "\n",
        "        for m in range(self.n_estimators):\n",
        "            # Compute residuals: r = y - F_{m-1}(x)\n",
        "            residuals = y - F_current\n",
        "\n",
        "            # Train a shallow regression tree to predict residuals\n",
        "            tree = DecisionTree(\n",
        "                classification=False,\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                min_samples_leaf=self.min_samples_leaf,\n",
        "                criterion=\"mse\",\n",
        "                verbose=False,\n",
        "                max_features_split=self.max_features_split,\n",
        "                max_thresholds=self.max_thresholds,\n",
        "            )\n",
        "            tree.fit(X, residuals)\n",
        "\n",
        "            self.trees.append(tree)\n",
        "\n",
        "            # Update F_current = F_current + lr * tree(X)\n",
        "            pred_residuals_np = tree.predict(X)  # numpy\n",
        "            pred_residuals = torch.from_numpy(pred_residuals_np).float().to(self.device)\n",
        "\n",
        "            F_current = F_current + self.learning_rate * pred_residuals\n",
        "\n",
        "            if self.verbose:\n",
        "                mse = torch.mean((y - F_current) ** 2).item()\n",
        "                print(f\"[GB] Stage {m+1}/{self.n_estimators}, train MSE={mse:.4f}\")\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"[GB] Finished training all stages.\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    # Predict for batch\n",
        "    def predict(self, X: torch.Tensor):\n",
        "        X = X.to(self.device)\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Start from initial prediction\n",
        "        F = torch.full((n_samples,), self.init_value_, device=self.device)\n",
        "\n",
        "        # Add contribution of each tree\n",
        "        for tree in self.trees:\n",
        "            pred_np = tree.predict(X)  # numpy\n",
        "            pred = torch.from_numpy(pred_np).float().to(self.device)\n",
        "            F = F + self.learning_rate * pred\n",
        "\n",
        "        return F.cpu().numpy()\n"
      ],
      "metadata": {
        "id": "HHSJS6tp7KDf"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#       One-vs-Rest Gradient Boosting for Classification\n",
        "\n",
        "class GradientBoostingOVRClassifier:\n",
        "    \"\"\"\n",
        "    One-vs-Rest multiclass classifier based on GradientBoostingRegressor.\n",
        "\n",
        "    For each class k, trains a separate GB regressor on targets:\n",
        "        y_k = 1 if (y == k) else 0\n",
        "    Prediction: choose class with highest regressor output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_classes: int,\n",
        "        n_estimators: int = 30,\n",
        "        learning_rate: float = 0.1,\n",
        "        max_depth: int = 2,\n",
        "        min_samples_split: int = 10,\n",
        "        min_samples_leaf: int = 5,\n",
        "        max_features_split: Optional[int] = None,\n",
        "        max_thresholds: int = 32,\n",
        "        random_state: int = 42,\n",
        "        verbose: bool = False,\n",
        "    ):\n",
        "        self.n_classes = n_classes\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features_split = max_features_split\n",
        "        self.max_thresholds = max_thresholds\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.device = DEVICE\n",
        "        self.models: List[GradientBoostingRegressor] = []\n",
        "\n",
        "    def fit(self, X: torch.Tensor, y: torch.Tensor):\n",
        "        \"\"\"\n",
        "        X: [n_samples, n_features], torch.float32\n",
        "        y: [n_samples], torch.long with values in [0..n_classes-1]\n",
        "        \"\"\"\n",
        "        X = X.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        self.models = []\n",
        "\n",
        "        for k in range(self.n_classes):\n",
        "            if self.verbose:\n",
        "                print(f\"[GB-OVR] Training class {k}/{self.n_classes-1} ...\")\n",
        "\n",
        "            # Build binary target: 1 for class k, 0 otherwise\n",
        "            y_k = (y == k).float()\n",
        "\n",
        "            gb_reg = GradientBoostingRegressor(\n",
        "                n_estimators=self.n_estimators,\n",
        "                learning_rate=self.learning_rate,\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                min_samples_leaf=self.min_samples_leaf,\n",
        "                max_features_split=self.max_features_split,\n",
        "                max_thresholds=self.max_thresholds,\n",
        "                random_state=self.random_state + k,  # different seed per class\n",
        "                verbose=self.verbose,\n",
        "            )\n",
        "\n",
        "            gb_reg.fit(X, y_k)\n",
        "            self.models.append(gb_reg)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"[GB-OVR] Finished training all class models.\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: torch.Tensor):\n",
        "        \"\"\"\n",
        "        X: [n_samples, n_features]\n",
        "        Returns: numpy array of shape [n_samples], predicted class indices.\n",
        "        \"\"\"\n",
        "        X = X.to(self.device)\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        if len(self.models) == 0:\n",
        "            raise RuntimeError(\"GB-OVR classifier has not been fitted yet.\")\n",
        "\n",
        "        # Collect predictions from each class-regressor\n",
        "        all_scores = []\n",
        "\n",
        "        for k, model in enumerate(self.models):\n",
        "            scores_k = model.predict(X)  # numpy [n_samples]\n",
        "            all_scores.append(scores_k)\n",
        "\n",
        "        # Shape: [n_classes, n_samples]\n",
        "        all_scores = np.stack(all_scores, axis=0)\n",
        "\n",
        "        # Choose class with highest score\n",
        "        pred_classes = np.argmax(all_scores, axis=0)\n",
        "        return pred_classes\n"
      ],
      "metadata": {
        "id": "tHCE6Ub5VMHf"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Work with dataset London-Bike"
      ],
      "metadata": {
        "id": "lR5eJrDZFIlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing steps"
      ],
      "metadata": {
        "id": "t9YWwf4LHxkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset from file CSV (London Bike)"
      ],
      "metadata": {
        "id": "5VPZ78KhFuQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = \"/content/london_merged.csv\"  # adjust if needed\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "df.head()\n",
        "\n",
        "# timestamp - time\n",
        "# cnt - count of a new bike shares\n",
        "# t1 - real temperature in C\n",
        "# t2 - temperature in C \"feels like\"\n",
        "# hum - humidity in percentage\n",
        "# wind_speed - wind speed in km/h\n",
        "# weather_code - category of the weather\n",
        "# is_holiday - boolean field - 1 holiday / 0 non holiday\n",
        "# is_weekend -  boolean field - 1 if the day is weekend\n",
        "# season - 0-spring ; 1-summer; 2-fall; 3-winter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "YOkPYD66FJO9",
        "outputId": "3fa06c0e-f44a-4bbf-a4e8-8fce2a8cc6d4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             timestamp  cnt   t1   t2    hum  wind_speed  weather_code  \\\n",
              "0  2015-01-04 00:00:00  182  3.0  2.0   93.0         6.0           3.0   \n",
              "1  2015-01-04 01:00:00  138  3.0  2.5   93.0         5.0           1.0   \n",
              "2  2015-01-04 02:00:00  134  2.5  2.5   96.5         0.0           1.0   \n",
              "3  2015-01-04 03:00:00   72  2.0  2.0  100.0         0.0           1.0   \n",
              "4  2015-01-04 04:00:00   47  2.0  0.0   93.0         6.5           1.0   \n",
              "\n",
              "   is_holiday  is_weekend  season  \n",
              "0         0.0         1.0     3.0  \n",
              "1         0.0         1.0     3.0  \n",
              "2         0.0         1.0     3.0  \n",
              "3         0.0         1.0     3.0  \n",
              "4         0.0         1.0     3.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c91345b7-1483-4c6a-89ea-b279b6266db6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>cnt</th>\n",
              "      <th>t1</th>\n",
              "      <th>t2</th>\n",
              "      <th>hum</th>\n",
              "      <th>wind_speed</th>\n",
              "      <th>weather_code</th>\n",
              "      <th>is_holiday</th>\n",
              "      <th>is_weekend</th>\n",
              "      <th>season</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015-01-04 00:00:00</td>\n",
              "      <td>182</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015-01-04 01:00:00</td>\n",
              "      <td>138</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>93.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2015-01-04 02:00:00</td>\n",
              "      <td>134</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>96.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2015-01-04 03:00:00</td>\n",
              "      <td>72</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015-01-04 04:00:00</td>\n",
              "      <td>47</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>6.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c91345b7-1483-4c6a-89ea-b279b6266db6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c91345b7-1483-4c6a-89ea-b279b6266db6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c91345b7-1483-4c6a-89ea-b279b6266db6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a7ab71f8-942a-490a-bb6a-13c9eb90b869\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a7ab71f8-942a-490a-bb6a-13c9eb90b869')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a7ab71f8-942a-490a-bb6a-13c9eb90b869 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"# season - 0-spring ; 1-summer; 2-fall; 3-winter\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2015-01-04 01:00:00\",\n          \"2015-01-04 04:00:00\",\n          \"2015-01-04 02:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cnt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 54,\n        \"min\": 47,\n        \"max\": 182,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          138,\n          47,\n          134\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"t1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5,\n        \"min\": 2.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3.0,\n          2.5,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"t2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.036822067666386,\n        \"min\": 0.0,\n        \"max\": 2.5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.0,\n          2.5,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.1304951684997055,\n        \"min\": 93.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          93.0,\n          96.5,\n          100.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wind_speed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.24037034920393,\n        \"min\": 0.0,\n        \"max\": 6.5,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          5.0,\n          6.5,\n          6.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weather_code\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8944271909999161,\n        \"min\": 1.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_holiday\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_weekend\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"season\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 3.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "choose features & target; preprocessing data\n",
        "* target: cnt"
      ],
      "metadata": {
        "id": "QqM5UuzEGo1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose target column (bike count)\n",
        "target_col = \"cnt\"\n",
        "\n",
        "# Choose a set of numeric and categorical features\n",
        "feature_cols = [\n",
        "    \"t1\",           # real temperature\n",
        "    \"t2\",           # feels-like temperature\n",
        "    \"hum\",          # humidity\n",
        "    \"wind_speed\",   # wind speed\n",
        "    \"weather_code\", # weather condition code\n",
        "    \"is_holiday\",\n",
        "    \"is_weekend\",\n",
        "    \"season\",\n",
        "]\n",
        "\n",
        "# Filter dataframe (drop rows with missing values just in case)\n",
        "df_model = df[feature_cols + [target_col]].dropna()\n",
        "\n",
        "X = df_model[feature_cols].values\n",
        "y = df_model[target_col].values\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRSDn2fjGWga",
        "outputId": "843ed16c-5ea3-4587-b4bd-38164fb8aad9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (17414, 8)\n",
            "y shape: (17414,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "split train & test samples"
      ],
      "metadata": {
        "id": "1eHEAdrDHBVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Train/test split (NumPy level)\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(\"Train size:\", X_train_np.shape[0])\n",
        "print(\"Test size :\", X_test_np.shape[0])\n",
        "#  Convert to torch tensors\n",
        "\n",
        "# Features as float32\n",
        "X_train = torch.from_numpy(X_train_np).float().to(DEVICE)\n",
        "X_test  = torch.from_numpy(X_test_np).float().to(DEVICE)\n",
        "\n",
        "# Regression targets as float32\n",
        "y_train = torch.from_numpy(y_train_np).float().to(DEVICE)\n",
        "y_test  = torch.from_numpy(y_test_np).float().to(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qUR9KzwG9V6",
        "outputId": "457b8622-837c-440a-f358-4fcf4b2d2b7a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 13931\n",
            "Test size : 3483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with DecisionTree"
      ],
      "metadata": {
        "id": "b3IFBr-5HsmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_reg = DecisionTree(\n",
        "    classification=False,\n",
        "    max_depth=6,           # you can tune this\n",
        "    min_samples_split=10,  # to avoid overfitting\n",
        "    min_samples_leaf=5,\n",
        "    criterion=\"mse\",\n",
        ")\n",
        "\n",
        "dt_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions on train and test (returned as NumPy arrays)\n",
        "y_train_pred_dt = dt_reg.predict(X_train)\n",
        "y_test_pred_dt  = dt_reg.predict(X_test)"
      ],
      "metadata": {
        "id": "_YzperrOHsD3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with RandomForest"
      ],
      "metadata": {
        "id": "jY3QVywQIBvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_reg = RandomForest(\n",
        "    classification=False,\n",
        "    n_trees=10,            # can be increased if not too slow\n",
        "    max_depth=6,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=5,\n",
        "    max_features=None,     # use all features\n",
        "    bootstrap=True,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred_rf = rf_reg.predict(X_train)\n",
        "y_test_pred_rf  = rf_reg.predict(X_test)"
      ],
      "metadata": {
        "id": "O3bFmsAzIBdb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with GradientBoosting"
      ],
      "metadata": {
        "id": "6Vz8lwWYIPLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gb_reg = GradientBoostingRegressor(\n",
        "    n_estimators=50,       # number of boosting stages\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,           # shallow trees as \"stumps\"\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=5,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "gb_reg.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred_gb = gb_reg.predict(X_train)\n",
        "y_test_pred_gb  = gb_reg.predict(X_test)"
      ],
      "metadata": {
        "id": "PBXarNeIHId5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metric comparison  "
      ],
      "metadata": {
        "id": "KcIgYJklKupU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== London Bike Regression: Comparison ===\")\n",
        "\n",
        "print(\"=== DecisionTree Regression metrics ===\")\n",
        "dt_train_metrics = regression_metrics(y_train_np, y_train_pred_dt)\n",
        "dt_test_metrics  = regression_metrics(y_test_np,  y_test_pred_dt)\n",
        "print(\"Train:\", dt_train_metrics)\n",
        "print(\"Test :\", dt_test_metrics)\n",
        "print()\n",
        "\n",
        "print(\"=== RandomForest Regression metrics ===\")\n",
        "rf_train_metrics = regression_metrics(y_train_np, y_train_pred_rf)\n",
        "rf_test_metrics  = regression_metrics(y_test_np,  y_test_pred_rf)\n",
        "print(\"Train:\", rf_train_metrics)\n",
        "print(\"Test :\", rf_test_metrics)\n",
        "print()\n",
        "\n",
        "print(\"=== GradientBoosting Regression metrics ===\")\n",
        "gb_train_metrics = regression_metrics(y_train_np, y_train_pred_gb)\n",
        "gb_test_metrics  = regression_metrics(y_test_np,  y_test_pred_gb)\n",
        "print(\"Train:\", gb_train_metrics)\n",
        "print(\"Test :\", gb_test_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGUWHqLQIYD2",
        "outputId": "294d9d67-0548-4a0b-e061-306f82b0a6e4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== London Bike Regression: Comparison ===\n",
            "=== DecisionTree Regression metrics ===\n",
            "Train: {'MAE': 642.2857025477268, 'MSE': 781641.5865371141, 'RMSE': 884.1049635292826, 'R2': 0.33317057474508704}\n",
            "Test : {'MAE': 667.2574478924839, 'MSE': 848428.6816567325, 'RMSE': 921.101884514809, 'R2': 0.29193936401733833}\n",
            "\n",
            "=== RandomForest Regression metrics ===\n",
            "Train: {'MAE': 633.4912978702639, 'MSE': 760149.9432867651, 'RMSE': 871.8657828397471, 'R2': 0.3515053977167065}\n",
            "Test : {'MAE': 658.6913439349541, 'MSE': 827007.7889622814, 'RMSE': 909.3996860359483, 'R2': 0.30981628311787146}\n",
            "\n",
            "=== GradientBoosting Regression metrics ===\n",
            "Train: {'MAE': 639.0373218876745, 'MSE': 770185.726058651, 'RMSE': 877.6022596020654, 'R2': 0.3429437303578755}\n",
            "Test : {'MAE': 658.3612724208914, 'MSE': 825637.3093925558, 'RMSE': 908.6458657764068, 'R2': 0.3109600240789222}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###     London-Bike\n",
        "\n",
        "## DecisionTree\n",
        "\n",
        "*     :  MSE/RMSE   R  .\n",
        "\n",
        "*           .\n",
        "\n",
        "*           .\n",
        "\n",
        "## RandomForest\n",
        "\n",
        "* RandomForest       DecisionTree: MSE  RMSE    , R .\n",
        "\n",
        "*        .\n",
        "\n",
        "*  bootstrap-        .\n",
        "\n",
        "## GradientBoosting\n",
        "\n",
        "* GradientBoosting   :  MSE/RMSE   R   .\n",
        "\n",
        "* Boosting       (residuals),     .\n",
        "\n",
        "*   learning_rate = 0.1, max_depth = 3, n_estimators = 50          .\n",
        "\n",
        "## :       \n",
        "### DecisionTree < RandomForest < GradientBoosting.\n"
      ],
      "metadata": {
        "id": "zvcIzmF0Lq1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Work with dataset CIFAR"
      ],
      "metadata": {
        "id": "BF9gpWmqPB1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing steps"
      ],
      "metadata": {
        "id": "UfOTBv3iRtWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load dataset"
      ],
      "metadata": {
        "id": "y8uJ1B93RlIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf cifar-10-python.tar.gz -C /content/\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VarTm8SfPzj6",
        "outputId": "f58e3bb3-8bec-4f6c-ed71-a645ffc401a8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar-10-batches-py  cifar-10-python.tar.gz  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "function load 1 batch CIFAR-10"
      ],
      "metadata": {
        "id": "iV-qCD_AQw1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "def load_cifar10_batch(batch_path: str):\n",
        "    \"\"\"\n",
        "    Load a single CIFAR-10 batch file.\n",
        "    Returns:\n",
        "        X : np.ndarray of shape [10000, 3072]\n",
        "        y : np.ndarray of shape [10000]\n",
        "    \"\"\"\n",
        "    with open(batch_path, \"rb\") as f:\n",
        "        batch = pickle.load(f, encoding=\"bytes\")\n",
        "\n",
        "    # b\"data\" has shape [10000, 3072] (32*32*3)\n",
        "    X = batch[b\"data\"]\n",
        "    # b\"labels\" is a list of length 10000\n",
        "    y = np.array(batch[b\"labels\"], dtype=np.int64)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def load_cifar10_train_test(root_dir: str):\n",
        "    \"\"\"\n",
        "    Load CIFAR-10 train (5 batches) and test set.\n",
        "    root_dir: directory with cifar-10-batches-py\n",
        "    \"\"\"\n",
        "    # Load training data from batches 1..5\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    for i in range(1, 6):\n",
        "        batch_path = os.path.join(root_dir, f\"data_batch_{i}\")\n",
        "        X_batch, y_batch = load_cifar10_batch(batch_path)\n",
        "        X_list.append(X_batch)\n",
        "        y_list.append(y_batch)\n",
        "\n",
        "    X_train = np.concatenate(X_list, axis=0)  # [50000, 3072]\n",
        "    y_train = np.concatenate(y_list, axis=0)  # [50000]\n",
        "\n",
        "    # Load test batch\n",
        "    test_batch_path = os.path.join(root_dir, \"test_batch\")\n",
        "    X_test, y_test = load_cifar10_batch(test_batch_path)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "metadata": {
        "id": "zTH4K17gQrEl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_score(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Simple accuracy: percentage of correct predictions.\n",
        "    \"\"\"\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    return np.mean(y_true == y_pred)"
      ],
      "metadata": {
        "id": "XMlVCu4vStT2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load all train + test CIFAR-10"
      ],
      "metadata": {
        "id": "zjVFHtJiQ7gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to CIFAR-10 python version\n",
        "cifar_root = \"/content/cifar-10-batches-py\"  # adjust if needed\n",
        "\n",
        "X_train_full, y_train_full, X_test_full, y_test_full = load_cifar10_train_test(cifar_root)\n",
        "\n",
        "print(\"CIFAR-10 full shapes:\")\n",
        "print(\"X_train_full:\", X_train_full.shape)\n",
        "print(\"y_train_full:\", y_train_full.shape)\n",
        "print(\"X_test_full :\", X_test_full.shape)\n",
        "print(\"y_test_full :\", y_test_full.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G1Aop8PrJp-",
        "outputId": "0fccd4ff-788f-4c3f-c6ad-6b4f692a9efa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFAR-10 full shapes:\n",
            "X_train_full: (50000, 3072)\n",
            "y_train_full: (50000,)\n",
            "X_test_full : (10000, 3072)\n",
            "y_test_full : (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose a smaller, balanced CIFAR-10 subset between classes, randomly controlled,"
      ],
      "metadata": {
        "id": "IITuUSbJT0gP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_balanced(X, y, n_per_class, random_state=42):\n",
        "    \"\"\"\n",
        "    Sample n_per_class examples for each class (0..9) from X, y.\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    classes = np.unique(y)\n",
        "    X_out = []\n",
        "    y_out = []\n",
        "\n",
        "    for c in classes:\n",
        "        idx = np.where(y == c)[0]\n",
        "        if len(idx) < n_per_class:\n",
        "            raise ValueError(f\"Not enough samples for class {c}\")\n",
        "        chosen = rng.choice(idx, size=n_per_class, replace=False)\n",
        "        X_out.append(X[chosen])\n",
        "        y_out.append(y[chosen])\n",
        "\n",
        "    X_out = np.vstack(X_out)\n",
        "    y_out = np.concatenate(y_out)\n",
        "    return X_out, y_out\n"
      ],
      "metadata": {
        "id": "tdYGceh_T1VK"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing data"
      ],
      "metadata": {
        "id": "c_90bSFbRzcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_train_per_class = 5000\n",
        "n_test_per_class  = 1000\n",
        "\n",
        "X_train_small, y_train_small = sample_balanced(\n",
        "    X_train_full, y_train_full,\n",
        "    n_per_class=n_train_per_class,\n",
        "    random_state=0,\n",
        ")\n",
        "X_test_small, y_test_small = sample_balanced(\n",
        "    X_test_full, y_test_full,\n",
        "    n_per_class=n_test_per_class,\n",
        "    random_state=1,\n",
        ")\n",
        "\n",
        "print(\"Small train:\", X_train_small.shape, y_train_small.shape)\n",
        "print(\"Small test :\", X_test_small.shape,  y_test_small.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkxut212Q61l",
        "outputId": "367773cc-be0b-4b19-f450-1c60fe14fa5d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small train: (50000, 3072) (50000,)\n",
            "Small test : (10000, 3072) (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======== Normalize to [0, 1] and convert to float32 ========\n",
        "\n",
        "X_train_small = X_train_small.astype(np.float32) / 255.0\n",
        "X_test_small  = X_test_small.astype(np.float32) / 255.0\n",
        "\n",
        "# ======== Convert to torch tensors ========\n",
        "\n",
        "X_train_cifar = torch.from_numpy(X_train_small).float().to(DEVICE)\n",
        "X_test_cifar  = torch.from_numpy(X_test_small).float().to(DEVICE)\n",
        "\n",
        "y_train_cifar = torch.from_numpy(y_train_small).long().to(DEVICE)\n",
        "y_test_cifar  = torch.from_numpy(y_test_small).long().to(DEVICE)"
      ],
      "metadata": {
        "id": "DcfGeTx5t6xN"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with DecisionTree"
      ],
      "metadata": {
        "id": "4K01ESpPT5YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_cifar = DecisionTree(\n",
        "    classification=True,\n",
        "    max_depth=12,\n",
        "    min_samples_split=15,\n",
        "    min_samples_leaf=10,\n",
        "    criterion=\"entropy\",\n",
        "    verbose=True,\n",
        "    max_features_split=128,\n",
        "    max_thresholds=48,\n",
        ")\n",
        "\n",
        "print(\"Training DecisionTree on CIFAR-10 balanced subset...\")\n",
        "dt_cifar.fit(X_train_cifar, y_train_cifar)\n",
        "\n",
        "y_train_pred_dt = dt_cifar.predict(X_train_cifar)\n",
        "y_test_pred_dt  = dt_cifar.predict(X_test_cifar)\n",
        "\n",
        "acc_train_dt = accuracy_score(y_train_small, y_train_pred_dt)\n",
        "acc_test_dt  = accuracy_score(y_test_small,  y_test_pred_dt)\n",
        "\n",
        "print(\"DecisionTree CIFAR-10 (balanced subset)\")\n",
        "print(\"Train accuracy:\", acc_train_dt)\n",
        "print(\"Test  accuracy:\", acc_test_dt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftzCXz_iCxtz",
        "outputId": "7e734c7c-e126-4cc6-cdc5-8765ef0a1b09"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training DecisionTree on CIFAR-10 balanced subset...\n",
            "[DT] Start building tree: n_samples=50000, n_features=3072\n",
            "[DT] Finished building tree, total nodes: 2783\n",
            "DecisionTree CIFAR-10 (balanced subset)\n",
            "Train accuracy: 0.3125\n",
            "Test  accuracy: 0.2458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with RandomForest"
      ],
      "metadata": {
        "id": "Zl-ufibqUKez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Typical choice for number of features per tree: sqrt(#features)\n",
        "n_features = X_train_cifar.shape[1]\n",
        "max_feats_for_rf = int(math.sqrt(n_features))  # ~ 55 for 3072 features\n",
        "\n",
        "rf_cifar = RandomForest(\n",
        "    classification=True,\n",
        "    n_trees=100,\n",
        "    max_depth=10,              # similar to DT, can try 1012\n",
        "    min_samples_split=30,\n",
        "    min_samples_leaf=10,\n",
        "    max_features=128,\n",
        "    bootstrap=True,\n",
        "    random_state=42,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(\"\\nTraining RandomForest on CIFAR-10 balanced subset...\")\n",
        "rf_cifar.fit(X_train_cifar, y_train_cifar)\n",
        "\n",
        "# Predictions on train and test\n",
        "y_train_pred_rf = rf_cifar.predict(X_train_cifar)\n",
        "y_test_pred_rf  = rf_cifar.predict(X_test_cifar)\n",
        "\n",
        "acc_train_rf = accuracy_score(y_train_small, y_train_pred_rf)\n",
        "acc_test_rf  = accuracy_score(y_test_small,  y_test_pred_rf)\n",
        "\n",
        "print(\"RandomForest CIFAR-10 (balanced subset)\")\n",
        "print(\"Train accuracy:\", acc_train_rf)\n",
        "print(\"Test  accuracy:\", acc_test_rf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moeQ95d7Umfe",
        "outputId": "013bde7b-c9a5-41c3-c7b0-133f9e534983"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training RandomForest on CIFAR-10 balanced subset...\n",
            "[RF] Start training forest with 100 trees, 50000 samples, 3072 features, max_features per tree = 128\n",
            "[RF] Training tree 1/100 ...\n",
            "[RF] Done tree 1/100\n",
            "[RF] Training tree 2/100 ...\n",
            "[RF] Done tree 2/100\n",
            "[RF] Training tree 3/100 ...\n",
            "[RF] Done tree 3/100\n",
            "[RF] Training tree 4/100 ...\n",
            "[RF] Done tree 4/100\n",
            "[RF] Training tree 5/100 ...\n",
            "[RF] Done tree 5/100\n",
            "[RF] Training tree 6/100 ...\n",
            "[RF] Done tree 6/100\n",
            "[RF] Training tree 7/100 ...\n",
            "[RF] Done tree 7/100\n",
            "[RF] Training tree 8/100 ...\n",
            "[RF] Done tree 8/100\n",
            "[RF] Training tree 9/100 ...\n",
            "[RF] Done tree 9/100\n",
            "[RF] Training tree 10/100 ...\n",
            "[RF] Done tree 10/100\n",
            "[RF] Training tree 11/100 ...\n",
            "[RF] Done tree 11/100\n",
            "[RF] Training tree 12/100 ...\n",
            "[RF] Done tree 12/100\n",
            "[RF] Training tree 13/100 ...\n",
            "[RF] Done tree 13/100\n",
            "[RF] Training tree 14/100 ...\n",
            "[RF] Done tree 14/100\n",
            "[RF] Training tree 15/100 ...\n",
            "[RF] Done tree 15/100\n",
            "[RF] Training tree 16/100 ...\n",
            "[RF] Done tree 16/100\n",
            "[RF] Training tree 17/100 ...\n",
            "[RF] Done tree 17/100\n",
            "[RF] Training tree 18/100 ...\n",
            "[RF] Done tree 18/100\n",
            "[RF] Training tree 19/100 ...\n",
            "[RF] Done tree 19/100\n",
            "[RF] Training tree 20/100 ...\n",
            "[RF] Done tree 20/100\n",
            "[RF] Training tree 21/100 ...\n",
            "[RF] Done tree 21/100\n",
            "[RF] Training tree 22/100 ...\n",
            "[RF] Done tree 22/100\n",
            "[RF] Training tree 23/100 ...\n",
            "[RF] Done tree 23/100\n",
            "[RF] Training tree 24/100 ...\n",
            "[RF] Done tree 24/100\n",
            "[RF] Training tree 25/100 ...\n",
            "[RF] Done tree 25/100\n",
            "[RF] Training tree 26/100 ...\n",
            "[RF] Done tree 26/100\n",
            "[RF] Training tree 27/100 ...\n",
            "[RF] Done tree 27/100\n",
            "[RF] Training tree 28/100 ...\n",
            "[RF] Done tree 28/100\n",
            "[RF] Training tree 29/100 ...\n",
            "[RF] Done tree 29/100\n",
            "[RF] Training tree 30/100 ...\n",
            "[RF] Done tree 30/100\n",
            "[RF] Training tree 31/100 ...\n",
            "[RF] Done tree 31/100\n",
            "[RF] Training tree 32/100 ...\n",
            "[RF] Done tree 32/100\n",
            "[RF] Training tree 33/100 ...\n",
            "[RF] Done tree 33/100\n",
            "[RF] Training tree 34/100 ...\n",
            "[RF] Done tree 34/100\n",
            "[RF] Training tree 35/100 ...\n",
            "[RF] Done tree 35/100\n",
            "[RF] Training tree 36/100 ...\n",
            "[RF] Done tree 36/100\n",
            "[RF] Training tree 37/100 ...\n",
            "[RF] Done tree 37/100\n",
            "[RF] Training tree 38/100 ...\n",
            "[RF] Done tree 38/100\n",
            "[RF] Training tree 39/100 ...\n",
            "[RF] Done tree 39/100\n",
            "[RF] Training tree 40/100 ...\n",
            "[RF] Done tree 40/100\n",
            "[RF] Training tree 41/100 ...\n",
            "[RF] Done tree 41/100\n",
            "[RF] Training tree 42/100 ...\n",
            "[RF] Done tree 42/100\n",
            "[RF] Training tree 43/100 ...\n",
            "[RF] Done tree 43/100\n",
            "[RF] Training tree 44/100 ...\n",
            "[RF] Done tree 44/100\n",
            "[RF] Training tree 45/100 ...\n",
            "[RF] Done tree 45/100\n",
            "[RF] Training tree 46/100 ...\n",
            "[RF] Done tree 46/100\n",
            "[RF] Training tree 47/100 ...\n",
            "[RF] Done tree 47/100\n",
            "[RF] Training tree 48/100 ...\n",
            "[RF] Done tree 48/100\n",
            "[RF] Training tree 49/100 ...\n",
            "[RF] Done tree 49/100\n",
            "[RF] Training tree 50/100 ...\n",
            "[RF] Done tree 50/100\n",
            "[RF] Training tree 51/100 ...\n",
            "[RF] Done tree 51/100\n",
            "[RF] Training tree 52/100 ...\n",
            "[RF] Done tree 52/100\n",
            "[RF] Training tree 53/100 ...\n",
            "[RF] Done tree 53/100\n",
            "[RF] Training tree 54/100 ...\n",
            "[RF] Done tree 54/100\n",
            "[RF] Training tree 55/100 ...\n",
            "[RF] Done tree 55/100\n",
            "[RF] Training tree 56/100 ...\n",
            "[RF] Done tree 56/100\n",
            "[RF] Training tree 57/100 ...\n",
            "[RF] Done tree 57/100\n",
            "[RF] Training tree 58/100 ...\n",
            "[RF] Done tree 58/100\n",
            "[RF] Training tree 59/100 ...\n",
            "[RF] Done tree 59/100\n",
            "[RF] Training tree 60/100 ...\n",
            "[RF] Done tree 60/100\n",
            "[RF] Training tree 61/100 ...\n",
            "[RF] Done tree 61/100\n",
            "[RF] Training tree 62/100 ...\n",
            "[RF] Done tree 62/100\n",
            "[RF] Training tree 63/100 ...\n",
            "[RF] Done tree 63/100\n",
            "[RF] Training tree 64/100 ...\n",
            "[RF] Done tree 64/100\n",
            "[RF] Training tree 65/100 ...\n",
            "[RF] Done tree 65/100\n",
            "[RF] Training tree 66/100 ...\n",
            "[RF] Done tree 66/100\n",
            "[RF] Training tree 67/100 ...\n",
            "[RF] Done tree 67/100\n",
            "[RF] Training tree 68/100 ...\n",
            "[RF] Done tree 68/100\n",
            "[RF] Training tree 69/100 ...\n",
            "[RF] Done tree 69/100\n",
            "[RF] Training tree 70/100 ...\n",
            "[RF] Done tree 70/100\n",
            "[RF] Training tree 71/100 ...\n",
            "[RF] Done tree 71/100\n",
            "[RF] Training tree 72/100 ...\n",
            "[RF] Done tree 72/100\n",
            "[RF] Training tree 73/100 ...\n",
            "[RF] Done tree 73/100\n",
            "[RF] Training tree 74/100 ...\n",
            "[RF] Done tree 74/100\n",
            "[RF] Training tree 75/100 ...\n",
            "[RF] Done tree 75/100\n",
            "[RF] Training tree 76/100 ...\n",
            "[RF] Done tree 76/100\n",
            "[RF] Training tree 77/100 ...\n",
            "[RF] Done tree 77/100\n",
            "[RF] Training tree 78/100 ...\n",
            "[RF] Done tree 78/100\n",
            "[RF] Training tree 79/100 ...\n",
            "[RF] Done tree 79/100\n",
            "[RF] Training tree 80/100 ...\n",
            "[RF] Done tree 80/100\n",
            "[RF] Training tree 81/100 ...\n",
            "[RF] Done tree 81/100\n",
            "[RF] Training tree 82/100 ...\n",
            "[RF] Done tree 82/100\n",
            "[RF] Training tree 83/100 ...\n",
            "[RF] Done tree 83/100\n",
            "[RF] Training tree 84/100 ...\n",
            "[RF] Done tree 84/100\n",
            "[RF] Training tree 85/100 ...\n",
            "[RF] Done tree 85/100\n",
            "[RF] Training tree 86/100 ...\n",
            "[RF] Done tree 86/100\n",
            "[RF] Training tree 87/100 ...\n",
            "[RF] Done tree 87/100\n",
            "[RF] Training tree 88/100 ...\n",
            "[RF] Done tree 88/100\n",
            "[RF] Training tree 89/100 ...\n",
            "[RF] Done tree 89/100\n",
            "[RF] Training tree 90/100 ...\n",
            "[RF] Done tree 90/100\n",
            "[RF] Training tree 91/100 ...\n",
            "[RF] Done tree 91/100\n",
            "[RF] Training tree 92/100 ...\n",
            "[RF] Done tree 92/100\n",
            "[RF] Training tree 93/100 ...\n",
            "[RF] Done tree 93/100\n",
            "[RF] Training tree 94/100 ...\n",
            "[RF] Done tree 94/100\n",
            "[RF] Training tree 95/100 ...\n",
            "[RF] Done tree 95/100\n",
            "[RF] Training tree 96/100 ...\n",
            "[RF] Done tree 96/100\n",
            "[RF] Training tree 97/100 ...\n",
            "[RF] Done tree 97/100\n",
            "[RF] Training tree 98/100 ...\n",
            "[RF] Done tree 98/100\n",
            "[RF] Training tree 99/100 ...\n",
            "[RF] Done tree 99/100\n",
            "[RF] Training tree 100/100 ...\n",
            "[RF] Done tree 100/100\n",
            "[RF] Finished training all trees.\n",
            "RandomForest CIFAR-10 (balanced subset)\n",
            "Train accuracy: 0.25946\n",
            "Test  accuracy: 0.2013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with Gradient Boostring"
      ],
      "metadata": {
        "id": "XVOF_QmnUnXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = 10\n",
        "\n",
        "gb_cifar = GradientBoostingOVRClassifier(\n",
        "    n_classes=n_classes,\n",
        "    n_estimators=20,        # keep small for speed\n",
        "    learning_rate=0.1,\n",
        "    max_depth=2,            # shallow trees like \"stumps\"\n",
        "    min_samples_split=20,\n",
        "    min_samples_leaf=10,\n",
        "    max_features_split=64,  # subset of features per split\n",
        "    max_thresholds=32,      # limit thresholds per feature\n",
        "    random_state=42,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(\"\\nTraining GradientBoosting OVR on CIFAR-10 balanced subset...\")\n",
        "gb_cifar.fit(X_train_cifar, y_train_cifar)\n",
        "\n",
        "y_train_pred_gb = gb_cifar.predict(X_train_cifar)\n",
        "y_test_pred_gb  = gb_cifar.predict(X_test_cifar)\n",
        "\n",
        "acc_train_gb = accuracy_score(y_train_small, y_train_pred_gb)\n",
        "acc_test_gb  = accuracy_score(y_test_small,  y_test_pred_gb)\n",
        "\n",
        "print(\"GradientBoosting OVR CIFAR-10\")\n",
        "print(\"Train accuracy:\", acc_train_gb)\n",
        "print(\"Test  accuracy:\", acc_test_gb)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQK4EgtqXrQ5",
        "outputId": "43514601-366d-462b-fd2a-fc05a37ea7a6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training GradientBoosting OVR on CIFAR-10 balanced subset...\n",
            "[GB-OVR] Training class 0/9 ...\n",
            "[GB] Start training: n_estimators=20, max_depth=2, lr=0.1\n",
            "[GB] Stage 1/20, train MSE=0.0898\n",
            "[GB] Stage 2/20, train MSE=0.0884\n",
            "[GB] Stage 3/20, train MSE=0.0881\n",
            "[GB] Stage 4/20, train MSE=0.0881\n",
            "[GB] Stage 5/20, train MSE=0.0877\n",
            "[GB] Stage 6/20, train MSE=0.0868\n",
            "[GB] Stage 7/20, train MSE=0.0866\n",
            "[GB] Stage 8/20, train MSE=0.0865\n",
            "[GB] Stage 9/20, train MSE=0.0859\n",
            "[GB] Stage 10/20, train MSE=0.0856\n",
            "[GB] Stage 11/20, train MSE=0.0854\n",
            "[GB] Stage 12/20, train MSE=0.0853\n",
            "[GB] Stage 13/20, train MSE=0.0852\n",
            "[GB] Stage 14/20, train MSE=0.0852\n",
            "[GB] Stage 15/20, train MSE=0.0850\n",
            "[GB] Stage 16/20, train MSE=0.0848\n",
            "[GB] Stage 17/20, train MSE=0.0845\n",
            "[GB] Stage 18/20, train MSE=0.0842\n",
            "[GB] Stage 19/20, train MSE=0.0837\n",
            "[GB] Stage 20/20, train MSE=0.0831\n",
            "[GB] Finished training all stages.\n",
            "[GB-OVR] Training class 1/9 ...\n",
            "[GB] Start training: n_estimators=20, max_depth=2, lr=0.1\n",
            "[GB] Stage 1/20, train MSE=0.0896\n",
            "[GB] Stage 2/20, train MSE=0.0893\n",
            "[GB] Stage 3/20, train MSE=0.0891\n",
            "[GB] Stage 4/20, train MSE=0.0890\n",
            "[GB] Stage 5/20, train MSE=0.0890\n",
            "[GB] Stage 6/20, train MSE=0.0889\n",
            "[GB] Stage 7/20, train MSE=0.0884\n",
            "[GB] Stage 8/20, train MSE=0.0883\n",
            "[GB] Stage 9/20, train MSE=0.0882\n",
            "[GB] Stage 10/20, train MSE=0.0877\n",
            "[GB] Stage 11/20, train MSE=0.0876\n",
            "[GB] Stage 12/20, train MSE=0.0875\n",
            "[GB] Stage 13/20, train MSE=0.0874\n",
            "[GB] Stage 14/20, train MSE=0.0870\n",
            "[GB] Stage 15/20, train MSE=0.0868\n",
            "[GB] Stage 16/20, train MSE=0.0867\n",
            "[GB] Stage 17/20, train MSE=0.0867\n",
            "[GB] Stage 18/20, train MSE=0.0866\n",
            "[GB] Stage 19/20, train MSE=0.0866\n",
            "[GB] Stage 20/20, train MSE=0.0862\n",
            "[GB] Finished training all stages.\n",
            "[GB-OVR] Training class 2/9 ...\n",
            "[GB] Start training: n_estimators=20, max_depth=2, lr=0.1\n",
            "[GB] Stage 1/20, train MSE=0.0899\n",
            "[GB] Stage 2/20, train MSE=0.0898\n",
            "[GB] Stage 3/20, train MSE=0.0897\n",
            "[GB] Stage 4/20, train MSE=0.0896\n",
            "[GB] Stage 5/20, train MSE=0.0896\n",
            "[GB] Stage 6/20, train MSE=0.0895\n",
            "[GB] Stage 7/20, train MSE=0.0894\n",
            "[GB] Stage 8/20, train MSE=0.0892\n",
            "[GB] Stage 9/20, train MSE=0.0891\n",
            "[GB] Stage 10/20, train MSE=0.0890\n",
            "[GB] Stage 11/20, train MSE=0.0889\n",
            "[GB] Stage 12/20, train MSE=0.0889\n",
            "[GB] Stage 13/20, train MSE=0.0888\n",
            "[GB] Stage 14/20, train MSE=0.0887\n",
            "[GB] Stage 15/20, train MSE=0.0887\n",
            "[GB] Stage 16/20, train MSE=0.0886\n",
            "[GB] Stage 17/20, train MSE=0.0886\n",
            "[GB] Stage 18/20, train MSE=0.0885\n",
            "[GB] Stage 19/20, train MSE=0.0885\n",
            "[GB] Stage 20/20, train MSE=0.0884\n",
            "[GB] Finished training all stages.\n",
            "[GB-OVR] Training class 3/9 ...\n",
            "[GB] Start training: n_estimators=20, max_depth=2, lr=0.1\n",
            "[GB] Stage 1/20, train MSE=0.0899\n",
            "[GB] Stage 2/20, train MSE=0.0898\n",
            "[GB] Stage 3/20, train MSE=0.0896\n",
            "[GB] Stage 4/20, train MSE=0.0894\n",
            "[GB] Stage 5/20, train MSE=0.0894\n",
            "[GB] Stage 6/20, train MSE=0.0892\n",
            "[GB] Stage 7/20, train MSE=0.0891\n",
            "[GB] Stage 8/20, train MSE=0.0890\n",
            "[GB] Stage 9/20, train MSE=0.0890\n",
            "[GB] Stage 10/20, train MSE=0.0889\n",
            "[GB] Stage 11/20, train MSE=0.0888\n",
            "[GB] Stage 12/20, train MSE=0.0888\n",
            "[GB] Stage 13/20, train MSE=0.0886\n",
            "[GB] Stage 14/20, train MSE=0.0885\n",
            "[GB] Stage 15/20, train MSE=0.0884\n",
            "[GB] Stage 16/20, train MSE=0.0884\n",
            "[GB] Stage 17/20, train MSE=0.0883\n",
            "[GB] Stage 18/20, train MSE=0.0882\n",
            "[GB] Stage 19/20, train MSE=0.0881\n",
            "[GB] Stage 20/20, train MSE=0.0881\n",
            "[GB] Finished training all stages.\n",
            "[GB-OVR] Training class 4/9 ...\n",
            "[GB] Start training: n_estimators=20, max_depth=2, lr=0.1\n",
            "[GB] Stage 1/20, train MSE=0.0898\n",
            "[GB] Stage 2/20, train MSE=0.0896\n",
            "[GB] Stage 3/20, train MSE=0.0894\n",
            "[GB] Stage 4/20, train MSE=0.0891\n",
            "[GB] Stage 5/20, train MSE=0.0890\n",
            "[GB] Stage 6/20, train MSE=0.0889\n",
            "[GB] Stage 7/20, train MSE=0.0887\n",
            "[GB] Stage 8/20, train MSE=0.0885\n",
            "[GB] Stage 9/20, train MSE=0.0884\n",
            "[GB] Stage 10/20, train MSE=0.0882\n",
            "[GB] Stage 11/20, train MSE=0.0881\n",
            "[GB] Stage 12/20, train MSE=0.0879\n",
            "[GB] Stage 13/20, train MSE=0.0876\n",
            "[GB] Stage 14/20, train MSE=0.0874\n",
            "[GB] Stage 15/20, train MSE=0.0873\n",
            "[GB] Stage 16/20, train MSE=0.0872\n",
            "[GB] Stage 17/20, train MSE=0.0871\n",
            "[GB] Stage 18/20, train MSE=0.0869\n",
            "[GB] Stage 19/20, train MSE=0.0867\n",
            "[GB] Stage 20/20, train MSE=0.0866\n",
            "[GB] Finished training all stages.\n",
            "[GB-OVR] Training class 5/9 ...\n",
            "[GB] Start training: n_estimators=20, max_depth=2, lr=0.1\n",
            "[GB] Stage 1/20, train MSE=0.0898\n",
            "[GB] Stage 2/20, train MSE=0.0896\n",
            "[GB] Stage 3/20, train MSE=0.0896\n",
            "[GB] Stage 4/20, train MSE=0.0895\n",
            "[GB] Stage 5/20, train MSE=0.0894\n",
            "[GB] Stage 6/20, train MSE=0.0892\n",
            "[GB] Stage 7/20, train MSE=0.0890\n",
            "[GB] Stage 8/20, train MSE=0.0889\n",
            "[GB] Stage 9/20, train MSE=0.0888\n",
            "[GB] Stage 10/20, train MSE=0.0886\n",
            "[GB] Stage 11/20, train MSE=0.0885\n",
            "[GB] Stage 12/20, train MSE=0.0884\n",
            "[GB] Stage 13/20, train MSE=0.0882\n",
            "[GB] Stage 14/20, train MSE=0.0881\n",
            "[GB] Stage 15/20, train MSE=0.0880\n",
            "[GB] Stage 16/20, train MSE=0.0879\n",
            "[GB] Stage 17/20, train MSE=0.0879\n",
            "[GB] Stage 18/20, train MSE=0.0878\n",
            "[GB] Stage 19/20, train MSE=0.0876\n",
            "[GB] Stage 20/20, train MSE=0.0873\n",
            "[GB] Finished training all stages.\n",
            "[GB-OVR] Training class 6/9 ...\n",
            "[GB] Start training: n_estimators=20, max_depth=2, lr=0.1\n",
            "[GB] Stage 1/20, train MSE=0.0899\n",
            "[GB] Stage 2/20, train MSE=0.0895\n",
            "[GB] Stage 3/20, train MSE=0.0894\n",
            "[GB] Stage 4/20, train MSE=0.0890\n",
            "[GB] Stage 5/20, train MSE=0.0885\n",
            "[GB] Stage 6/20, train MSE=0.0883\n",
            "[GB] Stage 7/20, train MSE=0.0882\n",
            "[GB] Stage 8/20, train MSE=0.0879\n",
            "[GB] Stage 9/20, train MSE=0.0878\n",
            "[GB] Stage 10/20, train MSE=0.0876\n",
            "[GB] Stage 11/20, train MSE=0.0873\n",
            "[GB] Stage 12/20, train MSE=0.0872\n",
            "[GB] Stage 13/20, train MSE=0.0871\n",
            "[GB] Stage 14/20, train MSE=0.0867\n",
            "[GB] Stage 15/20, train MSE=0.0866\n",
            "[GB] Stage 16/20, train MSE=0.0862\n",
            "[GB] Stage 17/20, train MSE=0.0861\n",
            "[GB] Stage 18/20, train MSE=0.0860\n",
            "[GB] Stage 19/20, train MSE=0.0860\n",
            "[GB] Stage 20/20, train MSE=0.0859\n",
            "[GB] Finished training all stages.\n",
            "[GB-OVR] Training class 7/9 ...\n",
            "[GB] Start training: n_estimators=20, max_depth=2, lr=0.1\n",
            "[GB] Stage 1/20, train MSE=0.0898\n",
            "[GB] Stage 2/20, train MSE=0.0896\n",
            "[GB] Stage 3/20, train MSE=0.0896\n",
            "[GB] Stage 4/20, train MSE=0.0895\n",
            "[GB] Stage 5/20, train MSE=0.0895\n",
            "[GB] Stage 6/20, train MSE=0.0894\n",
            "[GB] Stage 7/20, train MSE=0.0893\n",
            "[GB] Stage 8/20, train MSE=0.0892\n",
            "[GB] Stage 9/20, train MSE=0.0889\n",
            "[GB] Stage 10/20, train MSE=0.0888\n",
            "[GB] Stage 11/20, train MSE=0.0888\n",
            "[GB] Stage 12/20, train MSE=0.0887\n",
            "[GB] Stage 13/20, train MSE=0.0885\n",
            "[GB] Stage 14/20, train MSE=0.0884\n",
            "[GB] Stage 15/20, train MSE=0.0883\n",
            "[GB] Stage 16/20, train MSE=0.0881\n",
            "[GB] Stage 17/20, train MSE=0.0880\n",
            "[GB] Stage 18/20, train MSE=0.0879\n",
            "[GB] Stage 19/20, train MSE=0.0878\n",
            "[GB] Stage 20/20, train MSE=0.0877\n",
            "[GB] Finished training all stages.\n",
            "[GB-OVR] Training class 8/9 ...\n",
            "[GB] Start training: n_estimators=20, max_depth=2, lr=0.1\n",
            "[GB] Stage 1/20, train MSE=0.0893\n",
            "[GB] Stage 2/20, train MSE=0.0885\n",
            "[GB] Stage 3/20, train MSE=0.0880\n",
            "[GB] Stage 4/20, train MSE=0.0879\n",
            "[GB] Stage 5/20, train MSE=0.0869\n",
            "[GB] Stage 6/20, train MSE=0.0861\n",
            "[GB] Stage 7/20, train MSE=0.0853\n",
            "[GB] Stage 8/20, train MSE=0.0847\n",
            "[GB] Stage 9/20, train MSE=0.0844\n",
            "[GB] Stage 10/20, train MSE=0.0842\n",
            "[GB] Stage 11/20, train MSE=0.0840\n",
            "[GB] Stage 12/20, train MSE=0.0839\n",
            "[GB] Stage 13/20, train MSE=0.0838\n",
            "[GB] Stage 14/20, train MSE=0.0835\n",
            "[GB] Stage 15/20, train MSE=0.0832\n",
            "[GB] Stage 16/20, train MSE=0.0828\n",
            "[GB] Stage 17/20, train MSE=0.0827\n",
            "[GB] Stage 18/20, train MSE=0.0819\n",
            "[GB] Stage 19/20, train MSE=0.0817\n",
            "[GB] Stage 20/20, train MSE=0.0814\n",
            "[GB] Finished training all stages.\n",
            "[GB-OVR] Training class 9/9 ...\n",
            "[GB] Start training: n_estimators=20, max_depth=2, lr=0.1\n",
            "[GB] Stage 1/20, train MSE=0.0899\n",
            "[GB] Stage 2/20, train MSE=0.0898\n",
            "[GB] Stage 3/20, train MSE=0.0897\n",
            "[GB] Stage 4/20, train MSE=0.0895\n",
            "[GB] Stage 5/20, train MSE=0.0890\n",
            "[GB] Stage 6/20, train MSE=0.0889\n",
            "[GB] Stage 7/20, train MSE=0.0887\n",
            "[GB] Stage 8/20, train MSE=0.0882\n",
            "[GB] Stage 9/20, train MSE=0.0874\n",
            "[GB] Stage 10/20, train MSE=0.0873\n",
            "[GB] Stage 11/20, train MSE=0.0868\n",
            "[GB] Stage 12/20, train MSE=0.0866\n",
            "[GB] Stage 13/20, train MSE=0.0862\n",
            "[GB] Stage 14/20, train MSE=0.0861\n",
            "[GB] Stage 15/20, train MSE=0.0860\n",
            "[GB] Stage 16/20, train MSE=0.0855\n",
            "[GB] Stage 17/20, train MSE=0.0853\n",
            "[GB] Stage 18/20, train MSE=0.0850\n",
            "[GB] Stage 19/20, train MSE=0.0848\n",
            "[GB] Stage 20/20, train MSE=0.0847\n",
            "[GB] Finished training all stages.\n",
            "[GB-OVR] Finished training all class models.\n",
            "GradientBoosting OVR CIFAR-10\n",
            "Train accuracy: 0.29324\n",
            "Test  accuracy: 0.2908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metric comparison"
      ],
      "metadata": {
        "id": "EYjt7eW_XwmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== CIFAR-10 Accuracy Comparison (balanced subset) ===\")\n",
        "\n",
        "acc_train_dt = accuracy_score(y_train_small, y_train_pred_dt)\n",
        "acc_test_dt  = accuracy_score(y_test_small,  y_test_pred_dt)\n",
        "\n",
        "acc_train_rf = accuracy_score(y_train_small, y_train_pred_rf)\n",
        "acc_test_rf  = accuracy_score(y_test_small,  y_test_pred_rf)\n",
        "\n",
        "acc_train_gb = accuracy_score(y_train_small, y_train_pred_gb)\n",
        "acc_test_gb  = accuracy_score(y_test_small,  y_test_pred_gb)\n",
        "\n",
        "print(f\"DecisionTree   - Train: {acc_train_dt:.4f} | Test: {acc_test_dt:.4f}\")\n",
        "print(f\"RandomForest   - Train: {acc_train_rf:.4f} | Test: {acc_test_rf:.4f}\")\n",
        "print(f\"GradientBoost. - Train: {acc_train_gb:.4f} | Test: {acc_test_gb:.4f}\")"
      ],
      "metadata": {
        "id": "6WmQla3DX29t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e6228db-08ba-4456-cba3-c0d3435daee4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CIFAR-10 Accuracy Comparison (balanced subset) ===\n",
            "DecisionTree   - Train: 0.3125 | Test: 0.2458\n",
            "RandomForest   - Train: 0.2595 | Test: 0.2013\n",
            "GradientBoost. - Train: 0.2932 | Test: 0.2908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **    CIFAR-10**\n",
        "\n",
        "\n",
        "#### DecisionTree (0.31 train / 0.25 test)\n",
        "\n",
        "*       ,      .\n",
        "\n",
        "*    CIFAR-10   (3072 ),      -,          10 .\n",
        "\n",
        "*      ,    train > test,    .\n",
        "\n",
        "### RandomForest (0.26 train / 0.20 test)\n",
        "\n",
        "*           ,     -   .\n",
        "\n",
        "*         (underfitting):      ,    .\n",
        "\n",
        "*        ,          ~0.20.    :     ,     RandomForest   ,     .\n",
        "\n",
        "### GradientBoosting (0.29 train / 0.29 test)\n",
        "\n",
        "*    ,        ( residuals).\n",
        "\n",
        "*   One-vs-Rest          0/1,          .\n",
        "\n",
        "*     (n_estimators)        ,  train  test accuracy ,     (0.29)     .\n",
        "\n",
        "\n",
        "### ****: RandomForest < DecisionTree < GradientBoosting\n"
      ],
      "metadata": {
        "id": "5j-pG9XhN_Ng"
      }
    }
  ]
}