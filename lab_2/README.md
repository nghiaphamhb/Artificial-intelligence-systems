

# ğŸ§ª Lab 2 â€” K-Nearest Neighbors (KNN), Regularization, and Model Comparison

### *Artificial Intelligence Systems â€“ Laboratory Work #2*

---

## ğŸ“Œ 1. Objectives of the Lab

This lab consists of two major parts.

---

### **âœ” Part 1 â€” Extension of Lab 1 (Linear & Logistic Regression)**

You must:

* Reuse the **Linear Regression** and **Logistic Regression** models implemented from scratch in Lab 1.
* Add three types of regularization:

  * **L1 (Lasso)**
  * **L2 (Ridge)**
  * **ElasticNet**
* Train models using all four configurations:

  * No regularization
  * L1
  * L2
  * ElasticNet
* Plot **four curves on a single plot** showing:

  * Loss decrease over epochs
  * Metric improvement (RÂ² for regression, Accuracy/F1 for classification)

This part evaluates understanding of optimization, regularization, overfitting, and training dynamics.

---

### **âœ” Part 2 â€” Implementing K-Nearest Neighbors (KNN) From Scratch**

You must implement the KNN algorithm using:

* **NumPy (basic version)**
* **PyTorch tensors (advanced version)**
  â†’ *No high-level torch APIs (no nn.Module, no KDTree, no cdist). Only pure tensor operations.*

Your implementation must include:

```python
def __init__(...)   # model initialization
def fit(...)        # store training data
def predict(...)    # compute distances + KNN prediction
```

Must support **four distance metrics**:

| Metric    | Description           |
| --------- | --------------------- |
| Euclidean | L2 norm               |
| Manhattan | L1 norm               |
| Minkowski | General Lp norm       |
| Cosine    | 1 âˆ’ cosine similarity |

You will apply KNN to both:

1. **Classification:** Fashion-MNIST
2. **Regression:** US Health Insurance dataset

---

## ğŸ“š 2. Datasets

### ğŸ§µ Fashion-MNIST â€” Classification

* 60,000 training images
* 10,000 test images
* 28Ã—28 grayscale
* 10 clothing categories

Used to test KNN classification accuracy with different metrics.

---

### ğŸ’µ US Health Insurance â€” Regression

Target variable: **charges**
Features: age, sex, bmi, children, smoker, region

Used to evaluate KNN regression performance.

---

## ğŸ› ï¸ 3. Implementation Details

### âœ” **Part 1 â€” Linear & Logistic Regression with Regularization**

Implemented using **PyTorch tensors**, but:

* **No nn.Linear**
* **No autograd (gradient computed manually)**
* **No optimizers like Adam**

Regularized loss function:

[
L = MSE + \lambda (\alpha |w|_1 + (1 - \alpha)|w|_2^2)
]

You must plot:

* Training loss curves (4 lines)
* Validation loss curves (optional, but recommended)
* Training + validation metrics (RÂ² or Accuracy/F1)

---

### âœ” **Part 2 â€” KNN Implementation (NumPy â†’ PyTorch)**

#### NumPy version

Uses KDTree or cdist for distance computation (simple baseline).

#### PyTorch version (**required for full score**)

Distances computed manually:

```python
diff = x_batch.unsqueeze(1) - X_train.unsqueeze(0)
dist = (diff**2).sum(dim=2)
```

Supports batch processing to avoid memory overflow.

Runs on:

* **CPU**, or
* **GPU** automatically (`device="cuda"` if available)

---

## ğŸ“Š 4. Evaluation Metrics

### **Classification (Fashion-MNIST)**

You must report:

* **Accuracy**
* **Precision (weighted)**
* **Recall (weighted)**
* **F1-score**
* **Confusion matrix (heatmap)**

Each metric should be evaluated for all four distance functions.

---

### **Regression (Insurance dataset)**

You must report:

* MSE
* RMSE
* MAE
* Median Absolute Error
* RÂ² Score
* Explained Variance Score

Additionally:

* Plot **MSE across k = 1 â†’ 15**
  â†’ Helps analyze underfitting/overfitting trade-offs.

---

## ğŸ“ 5. Suggested Folder Structure

```
â”œâ”€â”€ README.md
â”œâ”€â”€ regularization/
â”‚   â”œâ”€â”€ linear_regression_torch.py
â”‚   â”œâ”€â”€ logistic_regression_torch.py
â”‚   â””â”€â”€ plots/
â”œâ”€â”€ knn/
â”‚   â”œâ”€â”€ knn_numpy.py
â”‚   â”œâ”€â”€ knn_torch_classifier.py
â”‚   â”œâ”€â”€ knn_torch_regressor.py
â”‚   â””â”€â”€ results/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ fashion-mnist_train.csv
â”‚   â”œâ”€â”€ fashion-mnist_test.csv
â”‚   â”œâ”€â”€ insurance.csv
â””â”€â”€ notebooks/
    â”œâ”€â”€ Lab2_KNN_Classification.ipynb
    â”œâ”€â”€ Lab2_KNN_Regression.ipynb
    â””â”€â”€ Lab2_Regularization.ipynb
```

---

## ğŸ§  6. Conclusion

At the end of this lab, students should be able to:

* Understand and implement L1, L2, and ElasticNet regularization.
* Build Linear/Logistic Regression from scratch in PyTorch using manual gradients.
* Implement KNN without relying on KDTree/cdist â€” using only tensor math.
* Evaluate both classification and regression tasks with appropriate metrics.
* Analyze how distance metrics and the value of K affect model performance.

